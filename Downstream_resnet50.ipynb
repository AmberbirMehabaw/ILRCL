{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf87af-4d5e-4a25-b1ae-1d44f51d1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from contrastive_pretrain_resnet50 import SimCLR, get_encoder_resnet50_cifar\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Frozen Encoder Wrapper\n",
    "# ============================================================\n",
    "\n",
    "class FrozenEncoder(nn.Module):\n",
    "    def __init__(self, ckpt_path, device):\n",
    "        super().__init__()\n",
    "        # Load full SimCLR model (encoder + projection)\n",
    "        self.model = SimCLR()\n",
    "        state = torch.load(ckpt_path, map_location=device)\n",
    "        self.model.load_state_dict(state)\n",
    "\n",
    "        # Keep ONLY the encoder, not the projector\n",
    "        self.encoder = self.model.encoder.to(device)\n",
    "        self.encoder.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)          # (B, 2048, 1, 1)\n",
    "        h = torch.flatten(h, 1)      # (B, 2048)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95031967-35cc-48b3-b254-f4d0a4c86ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. CIFAR-10 Transform (test-time)\n",
    "# ============================================================\n",
    "\n",
    "test_t = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Load CIFAR-10 Train/Test for Downstream\n",
    "# ============================================================\n",
    "\n",
    "def load_cifar(data_dir=\"./data\"):\n",
    "    train = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=test_t)\n",
    "    test  = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=test_t)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=256, shuffle=False)\n",
    "    test_loader  = DataLoader(test, batch_size=256, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Extract Embeddings\n",
    "# ============================================================\n",
    "\n",
    "def extract_embeddings(encoder, loader, device):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            h = encoder(x)\n",
    "            features.append(h.cpu())\n",
    "            labels.append(y)\n",
    "\n",
    "    return torch.cat(features), torch.cat(labels)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Classifier Head (MLP)\n",
    "# ============================================================\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim=2048, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Train Classifier\n",
    "# ============================================================\n",
    "\n",
    "def train_classifier(x_train, y_train, x_test, y_test, device, epochs=1000):\n",
    "    clf = Classifier(in_dim=x_train.size(1)).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(clf.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "    x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STORE METRICS\n",
    "    # ----------------------------------------\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_acc\": [],\n",
    "    }\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # -----------------------\n",
    "        # TRAIN FOR ONE EPOCH\n",
    "        # -----------------------\n",
    "        clf.train()\n",
    "        logits = clf(x_train)\n",
    "        loss = loss_fn(logits, y_train)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        train_preds = logits.argmax(dim=1)\n",
    "        train_acc = accuracy_score(y_train.cpu(), train_preds.cpu())\n",
    "\n",
    "        # -----------------------\n",
    "        # TEST EVAL THIS EPOCH\n",
    "        # -----------------------\n",
    "        clf.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = clf(x_test)\n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_preds = test_logits.argmax(dim=1)\n",
    "            test_acc = accuracy_score(y_test.cpu(), test_preds.cpu())\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # STORE IN HISTORY\n",
    "        # ----------------------------------------\n",
    "        history[\"train_loss\"].append(loss.item())\n",
    "        history[\"test_loss\"].append(test_loss.item())\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {ep+1}/{epochs}] \"\n",
    "                f\"TrainLoss={loss.item():.4f} TestLoss={test_loss.item():.4f} \"\n",
    "                f\"TrainAcc={train_acc:.4f} TestAcc={test_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    return clf, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c0c7c-9134-44aa-97cc-bb3db3a6e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. Evaluation: Accuracy, Precision, Recall, F1, AUC\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_classifier(clf, x_test, y_test, device):\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = clf(x_test.to(device))\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=1)\n",
    "\n",
    "    y_true = y_test.numpy()\n",
    "\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, preds, average=\"macro\")\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs, multi_class=\"ovr\")\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    print(\"\\n===== DOWNSTREAM RESULTS =====\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"AUC:       {auc:.4f}\" if auc is not None else \"AUC: N/A\")\n",
    "\n",
    "    return acc, prec, rec, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb567c-b7aa-4883-8a4a-67224f44fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. Embedding Visualization (optional)\n",
    "# ============================================================\n",
    "\n",
    "def visualize_embeddings(x, y, out_path=\"tsne_embeddings.png\", show=True):\n",
    "    x = x.numpy()\n",
    "    y = y.numpy()\n",
    "\n",
    "    tsne = TSNE(n_components=2, init='pca', learning_rate='auto')\n",
    "    z = tsne.fit_transform(x)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sc = plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10', s=4)\n",
    "    plt.colorbar(sc, ticks=range(10))\n",
    "    plt.title(\"t-SNE of Frozen Encoder Embeddings\")\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[t-SNE] Saved embedding visualization to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60e427-16d9-44fd-88f9-b3451b2f89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. plotting curves\n",
    "# ============================================================\n",
    "def plot_curves(history, out_path=\"downstream_curves.png\"):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # --- LOSS ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.legend()\n",
    "\n",
    "    # --- ACCURACY ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history[\"test_acc\"], label=\"Test Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Curves\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Plot] Saved learning curves to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074d1f1-33cc-4abe-9438-93e60df8fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. Main Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ckpt = \"simclr_resnet50_multicrop.pt\"   # pretrained checkpoint\n",
    "\n",
    "    print(\"Loading frozen encoder...\")\n",
    "    encoder = FrozenEncoder(ckpt, device)\n",
    "\n",
    "    print(\"Loading CIFAR-10 for downstream...\")\n",
    "    train_loader, test_loader = load_cifar()\n",
    "\n",
    "    print(\"Extracting embeddings...\")\n",
    "    x_train, y_train = extract_embeddings(encoder, train_loader, device)\n",
    "    x_test, y_test = extract_embeddings(encoder, test_loader, device)\n",
    "\n",
    "    print(\"Training classifier...\")\n",
    "    clf, history = train_classifier(x_train, y_train, x_test, y_test, device)\n",
    "    plot_curves(history)\n",
    "\n",
    "\n",
    "    print(\"Evaluating classifier...\")\n",
    "    evaluate_classifier(clf, x_test, y_test, device)\n",
    "\n",
    "    # Optional visualization\n",
    "    visualize_embeddings(x_test, y_test, out_path=\"tsne_test_embeddings1.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
